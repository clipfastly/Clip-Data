10.Given a bank customer, build a neural network-based classifier 
that can determine whether they will leave or not in the next 6 
months. Dataset Description: The case study is from an open-
source dataset from Kaggle. The dataset contains 10,000 sample 
points with 14 distinct features such as CustomerId, CreditScore, 
Geography, Gender, Age, Tenure, Balance, etc. Perform following 
steps: 
1. Read the dataset. 
2. Distinguish the feature and target set and divide the data set 
into training and test sets. 
3. Normalize the train and test data. 
4. Initialize and build the model. Identify the points of 
improvement and implement the same. 5. Print the accuracy 
score and confusion matrix (5 points) 
Program code:
# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.neural_network import MLPClassifier
# 1. Read the dataset
url = "C:/Users/palla/OneDrive/Desktop/Bank/Churn_Modelling.csv" # 
Local path to your dataset
dataset = pd.read_csv(url)
# 2. Check for missing values (optional)
print(dataset.isnull().sum()) # To check if there are any missing values
# 3. Handle categorical variables (Geography, Gender)
# Use one-hot encoding to convert categorical variables to numeric values
dataset = pd.get_dummies(dataset, columns=['Geography', 'Gender'], 
drop_first=True)
# 4. Distinguish the feature and target set
X = dataset.drop(columns=['Exited', 'CustomerId', 'Surname']) # Dropping 
non-useful columns like 'CustomerId' and 'Surname'
y = dataset['Exited'] # 'Exited' is the target variable (1 = Churn, 0 = No 
Churn)
# 5. Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, 
random_state=42)
# 6. Normalize the train and test data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# 7. Initialize and build the model using MLPClassifier
model = MLPClassifier(hidden_layer_sizes=(128, 64, 32), max_iter=1000, 
activation='relu', solver='adam', random_state=42)
# 8. Train the model
model.fit(X_train_scaled, y_train)
# 9. Predict on the test set
y_pred = model.predict(X_test_scaled)
# 10. Print the accuracy score
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")
# 11. Print the confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)
# 12. Visualization of the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No 
Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()


---------------------------------------------------------------------------------------------------------------

# %%
import numpy as np
import matplotlib.pyplot as plt
# %%
def func(x):
 return (x + 3) ** 2
# %%
def grad_func(x):
 return 2 * (x + 3)
# %%
# Gradient Descent Algorithm
def gradient_descent(starting_point, learning_rate, num_iterations):
 x = starting_point
 x_history = [x] # Keep track of x values for plotting
 for _ in range(num_iterations):
 # Calculate gradient
 gradient = grad_func(x)
 # Update x by moving against the gradient
 x = x - learning_rate * gradient
 # Append the new x value to history
 x_history.append(x)
 return x, x_history
# %%
# Parameters for gradient descent
starting_point = 2 # Starting point x = 2
learning_rate = 0.1 # Learning rate
num_iterations = 20 # Number of iterations
# %%
final_x, x_history = gradient_descent(starting_point, learning_rate, 
num_iterations)
# %%
print(f"Final x value after {num_iterations} iterations: {final_x}")
print(f"Minimum value of the function: {func(final_x)}")
# %%
x_vals = np.linspace(-10, 5, 100)
y_vals = func(x_vals)
# %%
plt.figure(figsize=(8, 6))
plt.plot(x_vals, y_vals, label='y = (x + 3)^2', color='blue')
plt.scatter(x_history, [func(x) for x in x_history], color='red', 
label='Gradient Descent Path')
plt.plot(x_history, [func(x) for x in x_history], color='red', 
linestyle='dashed', alpha=0.6)
plt.title('Gradient Descent to Find Local Minima')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.grid(True)
plt.show()

11.Implement Gradient Descent Algorithm to find the local minima of 
a function. For example, find the local minima of the function 
y=(x+3)Â² starting from the point x=2.

-----------------------------------------------------------------------------------------------------------------------------------------


# 12.Implement K-Nearest Neighbors algorithm on diabetes.csv
# dataset. Compute confusion matrix, accuracy, error rate, precision
# and recall on the given dataset.
# Program Code:

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score,precision_score, recall_score
# Step 1: Load the dataset
dataset = pd.read_csv("diabetes.csv")
# Step 2: Preprocess the data (fill missing values if any)
dataset.fillna(dataset.median(), inplace=True)
# Step 3: Define the feature set X and target variable y
X = dataset.drop("Outcome", axis=1) # Features
y = dataset["Outcome"] # Target variable
# Step 4: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
random_state=42)
# Step 5: Initialize and train the KNN model
knn = KNeighborsClassifier(n_neighbors=5) 
# You can experiment with different k-values
knn.fit(X_train, y_train)
# Step 6: Make predictions
y_pred = knn.predict(X_test)
# Step 7: Compute evaluation metrics
cm = confusion_matrix(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)
error_rate = 1 - accuracy
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
# Output results
print("Confusion Matrix:\n", cm)
print("Accuracy:", accuracy)
print("Error Rate:", error_rate)
print("Precision:", precision)
print("Recall:", recall)

------------------------------------------------------------------------------------------------------------------


# 13.Implement K-Means clustering/ hierarchical clustering on
# sales_data_sample.csv dataset. Determine the number of clusters using
# the elbow method.
# Program code:

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt # type: ignore
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
# Step 1: Load and Preprocess the Data
url = 'C:/Users/palla/OneDrive/Desktop/pro/k means/sales_data_sample.csv'
# Read the file with proper encoding
data = pd.read_csv(url,encoding='latin1') # Removed 'errors'argument
# Display column names to verify
print("Columns in Dataset:")
print(data.columns)
# Selecting relevant columns for clustering
# Update these column names based on your dataset structure
data_selected = data[['QUANTITYORDERED', 'PRICEEACH','SALES']] # Replace with actual column names
# Handle missing values by filling with the mean
data_selected = data_selected.fillna(data_selected.mean())
# Normalize the data (important for K-Means)
scaler = StandardScaler()
data_normalized = scaler.fit_transform(data_selected)
# Step 2: Determine the Optimal Number of Clusters using the Elbow Method (for K-Means)

inertia = []
for k in range(1, 11): # Trying k values from 1 to 10
 kmeans = KMeans(n_clusters=k, random_state=42)
 kmeans.fit(data_normalized)
 inertia.append(kmeans.inertia_)
# Plot the inertia to visualize the Elbow point
plt.figure(figsize=(8, 6))
plt.plot(range(1, 11), inertia, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia (Within-cluster sum of squares)')
plt.show()
# From the plot, look for the "elbow" point where inertia starts decreasing slowly
optimal_k = 4 # Replace with the value determined from the plot
# Step 3: K-Means Clustering
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
kmeans_labels = kmeans.fit_predict(data_normalized)
# Add the K-Means cluster labels to the dataset
data['KMeans_Cluster'] = kmeans_labels
# Step 4: Visualize the Clusters
plt.scatter(data['SALES'], data['QUANTITYORDERED'],
c=data['KMeans_Cluster'], cmap='viridis')
plt.title('K-Means Clustering (Sales vs Quantity Ordered)')
plt.xlabel('Sales')
plt.ylabel('Quantity Ordered')
plt.show()

------------------------------------------------------------------------------------------------------------------------

// Client Program

#include <netdb.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/socket.h>
#include <fcntl.h> // for open
#include <unistd.h> // for close
#include <arpa/inet.h>

#define MAX 80
#define PORT 8080
#define SA struct sockaddr
void func(int sockfd)
{
    char buff[MAX];
    int n;
    for (;;) {
        bzero(buff, sizeof(buff));
        printf("Enter the string : ");
        n = 0;
        while ((buff[n++] = getchar()) != '\n')
            ;
        write(sockfd, buff, sizeof(buff));
        bzero(buff, sizeof(buff));
        read(sockfd, buff, sizeof(buff));
        printf("From Server : %s", buff);
        if ((strncmp(buff, "exit", 4)) == 0) {
            printf("Client Exit...\n");
            break;
        }
    }
}
   
int main()
{
    int sockfd, connfd;
    struct sockaddr_in servaddr, cli;
   
    // socket create and verification
    sockfd = socket(AF_INET, SOCK_STREAM, 0);
    if (sockfd == -1) {
        printf("socket creation failed...\n");
        exit(0);
    }
    else
        printf("Socket successfully created..\n");
    bzero(&servaddr, sizeof(servaddr));
   
    // assign IP, PORT
    servaddr.sin_family = AF_INET;
    servaddr.sin_addr.s_addr = inet_addr("127.0.0.1");
    servaddr.sin_port = htons(PORT);
   
    // connect the client socket to server socket
    if (connect(sockfd, (SA*)&servaddr, sizeof(servaddr)) != 0) {
        printf("connection with the server failed...\n");
        exit(0);
    }
    else
        printf("connected to the server..\n");
   
    // function for chat
    func(sockfd);
   
    // close the socket
    close(sockfd);
}





